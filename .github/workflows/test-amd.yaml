name: Self-hosted runner AMD GPU (push)

on:
  push:
    branches:
      - main
      - ci-*


env:
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  OMP_NUM_THREADS: 8
  MKL_NUM_THREADS: 8
  PYTEST_TIMEOUT: 60
  TF_FORCE_GPU_ALLOW_GROWTH: true
  RUN_PT_TF_CROSS_TESTS: 1"
  HFGPU: "--device /dev/dri/renderD128"
  IMAGE: huggingface/transformers-pytorch-amd-gpu-push-ci

jobs:
  get_context:
    runs-on: [self-hosted, docker-gpu, amd-gpu, multi-amd-gpu, mi210]
    outputs:
      gpu: ${{steps.getgpu.outputs.gpu}}
      runner: ${{ runner.name }}
    steps:
      - id: "getgpu"
        run: |
          echo "Output runner ${{ runner.name }}"
          echo "Output gpu $GPU"
          echo "gpu=$GPU" >> $GITHUB_OUTPUT   
  check_runners:
    name: Check Runners    
    needs: [get_context]
    runs-on: [self-hosted, docker-gpu, amd-gpu, multi-amd-gpu, mi210]
    container:
      #image: huggingface/transformers-pytorch-amd-gpu-push-ci
      #image: ${{ github.workspace }}
      image: ${$IMAGE}
      options: --device /dev/kfd ${{ needs.get_context.outputs.gpu }} --shm-size "16gb" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - name: ROCM-SMI
        run: |
          rocm-smi