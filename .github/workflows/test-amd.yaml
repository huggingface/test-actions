name: Self-hosted runner AMD GPU (push)

on:
  push:
    branches:
      - main
      - ci-*


env:
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  OMP_NUM_THREADS: 8
  MKL_NUM_THREADS: 8
  PYTEST_TIMEOUT: 60
  TF_FORCE_GPU_ALLOW_GROWTH: true
  RUN_PT_TF_CROSS_TESTS: 1"
  HFGPU: "--device /dev/dri/renderD128"

jobs:
  get-gpu:
    runs-on: [self-hosted, docker-gpu, amd-gpu, multi-amd-gpu, mi210]
    outputs:
      image: ${{steps.my_step.outputs.image}}
    steps:
      - id: "my_step"
        run: |
          image="huggingface/transformers-pytorch-amd-gpu-push-ci"
          echo "Output [$image]"
          echo "image=$image" >> $GITHUB_OUTPUT   
  check_runners:
    name: Check Runners    
    needs: [get-gpu]
    runs-on: [self-hosted, docker-gpu, amd-gpu, multi-amd-gpu, mi210]
    container:
      image: ${{needs.get-gpu.outputs.image}}
      options: --device /dev/kfd ${{ needs.get-gpu.gpu }} --shm-size "16gb" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - name: ROCM-SMI
        run: |
          rocm-smi