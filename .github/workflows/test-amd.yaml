name: Self-hosted runner AMD GPU (push)

on:
  push:
    branches:
      - main
      - ci-*


env:
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  OMP_NUM_THREADS: 8
  MKL_NUM_THREADS: 8
  PYTEST_TIMEOUT: 60
  TF_FORCE_GPU_ALLOW_GROWTH: true
  RUN_PT_TF_CROSS_TESTS: 1
  hfgpu: "--device /dev/dri/renderD128"

jobs:

  check_runners:
    name: Check Runners     
    runs-on: [self-hosted, docker-gpu, amd-gpu, multi-amd-gpu, mi210]
    container:
      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now
      options: --device /dev/kfd ${{ env.hfgpu }} --shm-size "16gb" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - name: ROCM-SMI
        run: |
          rocm-smi